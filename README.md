# ğŸ™ï¸ AI Voice Assistant

This project is a **voice-enabled AI assistant** built with Python and Streamlit. It lets users **ask questions via speech**, get smart answers from **Metaâ€™s LLaMA-4 Maverick model** via OpenRouter API, and **hear responses spoken aloud** using `pyttsx3`. All interactions are logged to a **Google Sheet** for history tracking.

---

## ğŸš€ Features

âœ… Voice input using your microphone  
âœ… AI responses powered by `meta-llama/llama-4-maverick:free` via OpenRouter  
âœ… Text-to-speech playback using `pyttsx3`  
âœ… Conversation log saved to Google Sheets  
âœ… Streamlit-based responsive UI  
âœ… Sidebar with model/voice info and chat reset  
âœ… Secrets protected using `.env` and `.gitignore`

---

## ğŸ§  Tech Stack

| Feature        | Tool/Service                       |
|----------------|------------------------------------|
| Voice Input    | `speech_recognition` (Google API) |
| Voice Output   | `pyttsx3` (offline TTS engine)    |
| AI Model       | LLaMA-4 Maverick via OpenRouter   |
| UI             | `streamlit`                       |
| Logging        | Google Sheets API                 |
| Secret Mgmt    | `.env`, `python-dotenv`           |

---

## ğŸ“ Project Structure
```
voice_assistant/
â”‚
â”œâ”€â”€ .env # ğŸ”’ Secrets (API keys)
â”œâ”€â”€ app.py # ğŸš€ Main Streamlit app
â”œâ”€â”€ config.py # ğŸ”§ Loads environment variables
â”œâ”€â”€ credentials.json # ğŸ”‘ Google Sheets credentials (ignored in git)
â”œâ”€â”€ gpt_service.py # ğŸ§  Handles AI response from OpenRouter
â”œâ”€â”€ logger.py # ğŸ“Š Logs interactions to Google Sheets
â”œâ”€â”€ speech_module.py # ğŸ¤ Mic input and speech output
â”œâ”€â”€ requirements.txt # ğŸ“¦ Python dependencies
â””â”€â”€ venv/ # ğŸ“ Virtual environment (ignored)
```

---

## ğŸ” Environment Setup

Create a `.env` file with your keys:

```env
OPENROUTER_API_KEY=your_openrouter_api_key
GOOGLE_SHEET_ID=your_google_sheet_id
```

Note: Do NOT commit .env or credentials.json to GitHub. They are ignored in .gitignore.

## ğŸ“¦ Installation & Running
Clone this repo:
```
git clone https://github.com/DattatrayBodake25/ai-voice-assistant.git
cd ai-voice-assistant
```

Set up virtual environment:
```
python -m venv venv
venv\Scripts\activate   # On Windows
```
Install dependencies:
```
pip install -r requirements.txt
```

Add your .env and credentials.json files.

Run the app:
```
streamlit run app.py
```

## ğŸ“ How It Works (End-to-End)

User speaks â†’ speech converted to text using speech_recognition

Text sent to OpenRouter API â†’ response from LLaMA-4 model

Assistant replies:

- ğŸ“„ Printed on screen in chat UI

- ğŸ”Š Read out loud via pyttsx3

Interaction logged â†’ logger.py appends to Google Sheet with timestamp

## ğŸ–¼ï¸ UI Preview
<img src="https://via.placeholder.com/700x400.png?text=Streamlit+UI+Placeholder" alt="UI Screenshot" />


## â˜ï¸ Deployment Options
Run locally with Streamlit

Package as .exe with pyinstaller

Deploy to a server using Streamlit Sharing or Render

âš ï¸ Security & Git Tips
âœ… .env and credentials.json are excluded via .gitignore
âŒ Never push secrets or service account files to GitHub
âœ… Use environment variables and python-dotenv to load them safely


ğŸ’¡ Future Ideas
ğŸ”„ Continuous listening mode

ğŸ§  Support for multiple AI models

ğŸŒ Multilingual voice output

ğŸ›ï¸ Settings panel for user control

